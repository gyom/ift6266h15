
# With tips from 
#
# https://github.com/lisa-lab/pylearn2/blob/master/pylearn2/scripts/tutorials/multilayer_perceptron/mlp_tutorial_part_3.yaml
#

!obj:pylearn2.train.Train {
    dataset: &train !obj:ift6266h15.code.pylearn2.datasets.variable_image_dataset.DogsVsCats {
        transformer: &transformer !obj:ift6266h15.code.pylearn2.datasets.variable_image_dataset.RandomCrop {
            scaled_size: 256,
            crop_size: 221,
        },
        start: 0,
        stop: 20000,
    },
    model: !obj:pylearn2.models.mlp.MLP {
        input_space: !obj:pylearn2.space.Conv2DSpace {
            shape: [221, 221],
            num_channels: 3,
            },
        layers: [!obj:pylearn2.models.mlp.ConvRectifiedLinear {
                     layer_name: 'h0',
                     output_channels: 128,
                     irange: .05,
                     kernel_shape: [5, 5],
                     pool_shape: [3, 3],
                     pool_stride: [3, 3]
                 },
                 !obj:pylearn2.models.mlp.ConvRectifiedLinear {
                     layer_name: 'h1',
                     output_channels: 64,
                     irange: .05,
                     kernel_shape: [4, 4],
                     pool_shape: [3, 3],
                     pool_stride: [3, 3]
                 },
                 !obj:pylearn2.models.mlp.ConvRectifiedLinear {
                     layer_name: 'h2',
                     output_channels: 64,
                     irange: .05,
                     kernel_shape: [3, 3],
                     pool_shape: [2, 2],
                     pool_stride: [2, 2]
                 },
                 !obj:pylearn2.models.mlp.RectifiedLinear {
                     layer_name: 'h3',
                     dim: 256,
                     sparse_init: 10
                 },
                 !obj:pylearn2.models.mlp.RectifiedLinear {
                     layer_name: 'h4',
                     dim: 256,
                     sparse_init: 10
                 },
            !obj:pylearn2.models.mlp.Softmax {
                layer_name: 'y',
                n_classes: 2,
                irange: 0.01,
            },
        ],
    },
    algorithm: !obj:pylearn2.training_algorithms.sgd.SGD {
        batch_size: &batch_size 128,
        train_iteration_mode: 'batchwise_shuffled_sequential',
        batches_per_iter: 10,
        monitoring_batch_size: *batch_size,
        monitoring_batches: 10,
        monitor_iteration_mode: 'batchwise_shuffled_sequential',
        learning_rate: 1e-3,
        learning_rule: !obj:pylearn2.training_algorithms.learning_rule.Momentum {
            init_momentum: 0.95
        },
        monitoring_dataset: {
            'train' : *train,
            'valid': !obj:ift6266h15.code.pylearn2.datasets.variable_image_dataset.DogsVsCats {
                transformer: *transformer,
                start: 20000,
                stop: 25000,
            },
        },
        cost: !obj:pylearn2.costs.cost.MethodCost {
            method: 'cost_from_X',
        },
        termination_criterion: !obj:pylearn2.termination_criteria.EpochCounter {
            max_epochs: 100
        },
    },
    extensions: [
        !obj:pylearn2.train_extensions.best_params.MonitorBasedSaveBest {
             channel_name: 'valid_y_misclass',
             save_path: "/home/gyomalin/ML/tmp/base_conv_03_mlp_best.pkl"
        },
    ],
    save_path: "/home/gyomalin/ML/tmp/base_conv_03_mlp.pkl",
    save_freq: 1
}




